{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2247ff7-cb67-4ddd-8a5d-d1d77eb16144",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn\n",
    "org.clustering4ever core_2.11 0.9.8\n",
    "org.clustering4ever clusteringspark_2.11 0.9.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab94441a-3c85-46b4-b11e-56339384b318",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%classpath add mvn\n",
    "org.apache.spark spark-core_2.12 2.4.5\n",
    "org.apache.spark spark-streaming_2.12 2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.clustering4ever.spark.clustering.s2gstream.S2gstream\r\n",
       "import org.apache.spark.{SparkConf, SparkContext}\r\n",
       "import org.apache.spark.streaming.{Milliseconds, StreamingContext}\r\n",
       "import org.apache.spark.streaming.dstream.DStream\r\n",
       "import org.apache.log4j.Logger\r\n",
       "import org.apache.log4j.Level\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.clustering4ever.spark.clustering.s2gstream.S2gstream\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.streaming.{Milliseconds, StreamingContext}\n",
    "import org.apache.spark.streaming.dstream.DStream\n",
    "import org.apache.log4j.Logger\n",
    "import org.apache.log4j.Level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val master = \"local[2]\"\n",
    "val dirData = \"streams\"\n",
    "val dirSortie = \"results\"\n",
    "val path =  \"C://Users/ATTAOUI/Documents/SS_GStream/resources/DS1.txt\"\n",
    "val separator = \",\"\n",
    "val decayFactor = 0.99\n",
    "val lambdaAge = 1.2\n",
    "val nbNodesToAdd = 3\n",
    "val nbWind = 60\n",
    "var batchNumber = 1\n",
    "var batchIndex = 0\n",
    "val batchSize = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@73db797f"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkConf = new SparkConf().setAppName(this.getClass().getName())\n",
    "  sparkConf.setMaster(master).set(\"spark.driver.allowMultipleContexts\", \"true\")\n",
    "val sc = new SparkContext(sparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createDstream: (ssc: org.apache.spark.streaming.StreamingContext, file: String, separator: String)org.apache.spark.streaming.dstream.DStream[Array[Double]]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DStreams that reads batch files from dirData\n",
    "@transient def createDstream(ssc : StreamingContext, file: String, separator: String): DStream[Array[Double]] ={\n",
    "  val stream = ssc.textFileStream(file).map(x => x.split(separator).map(_.toDouble))\n",
    "  return stream\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.streaming.dstream.MappedDStream@577578cd"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val interval = 100\n",
    "val batchInterval = Milliseconds(interval)\n",
    "val ssc = new StreamingContext(sc, batchInterval)\n",
    "val stream = createDstream(ssc, dirData, separator)\n",
    "stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[4] at parallelize at <console>:122"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = sc.textFile(path).map(x => x.split(',').map(_.toDouble))\n",
    "val v = data.take(1)(0).length\n",
    "val data2 = data.collect()\n",
    "var labels =  data.map(x => x(v - 1).toInt).collect()\n",
    "val points2 = sc.parallelize(data2.slice(0, 2)) //sc.textFile(\"resources/nodes2.txt\").map(x => x.split(separator).map(_.toDouble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.clustering4ever.spark.streamclustering.streamData\n",
    "\n",
    "val labId = 2 //TODO: change -1 to -2 when you add the id to the file (last column) //-2 because the last 2 columns represent label & id\n",
    "val dim = points2.take(1)(0).size - labId\n",
    "var gstream = new S2gstream()\n",
    "    .setDecayFactor(decayFactor)\n",
    "    .setLambdaAge(lambdaAge)\n",
    "    .setMaxInsert(nbNodesToAdd)\n",
    "  // converting each point into an object\n",
    "val dstreamObj = stream.map( e =>\n",
    "    gstream.model.pointToObjet(e, dim, labId)\n",
    ")\n",
    "var stream2 = new Array[streamData](0)\n",
    "while(batchNumber <= (data2.length / batchSize) ){\n",
    "    val batch = data2.slice(batchIndex, batchIndex + batchSize)\n",
    "    stream2 :+= new streamData(batch.map(a => gstream.model.pointToObjet(a, dim, labId)))\n",
    "    batchIndex += batchSize\n",
    "    batchNumber += 1\n",
    "}\n",
    "val streamRDD = stream2.map(x => sc.parallelize(x.stream))\n",
    "dstreamObj.cache() //TODO: to save in memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "node: 1 -> 122.0, 199.0\n",
       "node: 2 -> 243.0, 434.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// initialization of the model by creating a graph of two nodes (the first 2 data-points)\n",
    "gstream.initModelObj(points2, dim)\n",
    "\n",
    "// training on the model\n",
    "val trainedModel = gstream.trainOnObj(dstreamObj, gstream, dirSortie + \"/dataset\" + nbNodesToAdd, dim, nbWind, streamRDD, labels, data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
